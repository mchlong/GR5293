{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRRvBdrYPGjt",
        "outputId": "c03e1eac-9883-4127-81db-1c818a975442"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==4.40.1 in /usr/local/lib/python3.11/dist-packages (4.40.1)\n",
            "Requirement already satisfied: peft==0.5.0 in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.1) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.5.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.5.0) (2.5.1+cu124)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from peft==0.5.0) (1.3.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.1) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.1) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.5.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.5.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.5.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.5.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.5.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.5.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.5.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.5.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.5.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.5.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.5.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.5.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.5.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.5.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.5.0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.5.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.5.0) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.1) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.1) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.5.0) (3.0.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.28.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.3)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.40.1 peft==0.5.0\n",
        "!pip install sentencepiece\n",
        "!pip install accelerate\n",
        "!pip install torch\n",
        "!pip install datasets\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_gmdxCnRPGju"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Log in to Hugging Face with your token\n",
        "login(\"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\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GJ4T0B5joLD",
        "outputId": "8dd3ee9a-0209-4be9-8e2f-34719bdad641"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDASrVzxPGju"
      },
      "source": [
        "***Model preparation***"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load model directly\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n"
      ],
      "metadata": {
        "id": "ODLRf65DWvV7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275,
          "referenced_widgets": [
            "96d3ed4927f842f38644a65d4fbae971",
            "0f0b69ec784a43fa9b2589aeb1592a15",
            "426254dc10734a26b81c965c4e3af6ea",
            "64023af9d137451fbbfd5fd6cd4d7a3a",
            "9a202611185542f290cdbec6fc4ebfdb",
            "1fcaf3432e1b4563aa0698ac1df0d2d5",
            "fa4851ee3d094a0386ddddd8815d1bb7",
            "0e55f713fe124503b5ad9cda848e2b0d",
            "fc8bfd64144e46f0a13450e402d56b27",
            "6e81828912a944c18cf64b2e2f09fb63",
            "24a5133c905c4765911d0dc91ae7599d"
          ]
        },
        "id": "VYpydTtSPGju",
        "outputId": "c5b36d0f-d840-4452-8128-ff754a27d1f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
            "The class this function is called from is 'LlamaTokenizerFast'.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96d3ed4927f842f38644a65d4fbae971"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import LlamaForCausalLM, LlamaTokenizerFast\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# Base model and PEFT (LoRA) model\n",
        "base_model = \"meta-llama/Meta-Llama-3-8B\"\n",
        "peft_model = \"FinGPT/fingpt-mt_llama3-8b_lora\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = LlamaTokenizerFast.from_pretrained(\n",
        "    base_model,\n",
        "    token=\"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\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load base model with 16-bit precision\n",
        "model = LlamaForCausalLM.from_pretrained(base_model,\n",
        "                    trust_remote_code=True,\n",
        "                    device_map=\"auto\",\n",
        "                    torch_dtype=torch.float16)  # Enable 16-bit precision\n",
        "\n",
        "# Apply LoRA-based PEFT model\n",
        "model = PeftModel.from_pretrained(model, peft_model, torch_dtype=torch.float16)\n",
        "model = model.eval()\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "feuvSUzvPGjv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# -----------------------\n",
        "# 1. PREP: MODEL + TOKENIZER\n",
        "# -----------------------\n",
        "# Make sure you've already defined or imported:\n",
        "#   tokenizer, model, device\n",
        "# from your previous code.\n",
        "# If you need them in this script, define them here similarly.\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Example sentiment function based on your reference\n",
        "def get_sentiment(text):\n",
        "    \"\"\"Return (sentiment_label, sentiment_probability).\"\"\"\n",
        "    if not text:\n",
        "        # If empty text, treat as neutral\n",
        "        return \"Neutral\", 1.0\n",
        "\n",
        "    # (Optional) Truncate very long text to avoid GPU OOM\n",
        "    len_text = min(len(text), 5000)\n",
        "    text = text[:len_text]\n",
        "\n",
        "    # Create prompt\n",
        "    prompt = (\n",
        "        f\"Instruction: What is the sentiment of this news? \"\n",
        "        f\"Please choose an answer from [Positive, Negative, Neutral].\\n\"\n",
        "        f\"Input: {text}\\nAnswer: \"\n",
        "    )\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, max_length=128).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Extract logits for the last token\n",
        "    logits = outputs.logits[:, -1, :].to(\"cpu\")\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    # Map the probabilities to the sentiment tokens\n",
        "    class_tokens = tokenizer([\"Positive\", \"Negative\", \"Neutral\"], add_special_tokens=False)[\"input_ids\"]\n",
        "    class_probs = {tokenizer.decode(token_id): probs[0, token_id].item() for token_id in class_tokens}\n",
        "\n",
        "    # Pick the highest-probability sentiment\n",
        "    sentiment_label = max(class_probs, key=class_probs.get)\n",
        "    sentiment_prob = class_probs[sentiment_label]\n",
        "\n",
        "    # Clean up\n",
        "    del inputs, outputs, logits, probs\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return sentiment_label, sentiment_prob\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# 2. AGGREGATION FUNCTION\n",
        "# -----------------------\n",
        "def aggregate_daily_sentiment(group: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Given a subset of rows (one day of articles),\n",
        "    compute the required metrics.\n",
        "    \"\"\"\n",
        "    total_articles = len(group)\n",
        "    if total_articles == 0:\n",
        "        return pd.Series({\n",
        "            \"mode_of_sentiment\": None,\n",
        "            \"num_articles\": 0,\n",
        "            \"ratio_positive\": 0.0,\n",
        "            \"ratio_negative\": 0.0,\n",
        "            \"ratio_neutral\": 0.0,\n",
        "            \"avg_sentiment_positive\": 0.0,\n",
        "            \"avg_sentiment_negative\": 0.0,\n",
        "            \"avg_sentiment_neutral\": 0.0,\n",
        "            \"weighted_avg_sentiment\": 0.0,\n",
        "            \"mode_of_sentiment_score\": None,\n",
        "            \"mode_of_avg_sentiment_score\": None\n",
        "        })\n",
        "\n",
        "    # 1) Mode of sentiment types (which label appears most)\n",
        "    sentiment_counts = group[\"sentiment\"].value_counts()\n",
        "    mode_of_sentiment = sentiment_counts.idxmax()\n",
        "\n",
        "    # 2) Number of articles\n",
        "    num_articles = total_articles\n",
        "\n",
        "    # 3) Ratio of positive/negative/neutral\n",
        "    ratio_positive = sentiment_counts.get(\"Positive\", 0) / total_articles\n",
        "    ratio_negative = sentiment_counts.get(\"Negative\", 0) / total_articles\n",
        "    ratio_neutral  = sentiment_counts.get(\"Neutral\",  0) / total_articles\n",
        "\n",
        "    # 4) Average sentiment for positive, negative, neutral\n",
        "    avg_sentiment_positive = group.loc[group[\"sentiment\"] == \"Positive\", \"sentiment_logit\"].mean()\n",
        "    avg_sentiment_negative = group.loc[group[\"sentiment\"] == \"Negative\", \"sentiment_logit\"].mean()\n",
        "    avg_sentiment_neutral  = group.loc[group[\"sentiment\"] == \"Neutral\",  \"sentiment_logit\"].mean()\n",
        "\n",
        "    # 5) Weighted average sentiment:\n",
        "    #    (sum of all positive sentiments - sum of all negative sentiments) / total_articles\n",
        "    sum_pos = group.loc[group[\"sentiment\"] == \"Positive\", \"sentiment_logit\"].sum()\n",
        "    sum_neg = group.loc[group[\"sentiment\"] == \"Negative\", \"sentiment_logit\"].sum()\n",
        "    weighted_avg_sentiment = (sum_pos - sum_neg) / total_articles\n",
        "\n",
        "    # 6) Mode of sentiment score (which sentiment label has the highest sum of probabilities)\n",
        "    sum_sentiment_scores = group.groupby(\"sentiment\")[\"sentiment_logit\"].sum()\n",
        "    mode_of_sentiment_score = sum_sentiment_scores.idxmax() if not sum_sentiment_scores.empty else None\n",
        "\n",
        "    # 7) Mode of average sentiment score (which label has the highest average probability)\n",
        "    avg_sentiment_scores = group.groupby(\"sentiment\")[\"sentiment_logit\"].mean()\n",
        "    mode_of_avg_sentiment_score = avg_sentiment_scores.idxmax() if not avg_sentiment_scores.empty else None\n",
        "\n",
        "    return pd.Series({\n",
        "        \"mode_of_sentiment\": mode_of_sentiment,\n",
        "        \"num_articles\": num_articles,\n",
        "        \"ratio_positive\": ratio_positive,\n",
        "        \"ratio_negative\": ratio_negative,\n",
        "        \"ratio_neutral\":  ratio_neutral,\n",
        "        \"avg_sentiment_positive\": avg_sentiment_positive,\n",
        "        \"avg_sentiment_negative\": avg_sentiment_negative,\n",
        "        \"avg_sentiment_neutral\":  avg_sentiment_neutral,\n",
        "        \"weighted_avg_sentiment\": weighted_avg_sentiment,\n",
        "        \"mode_of_sentiment_score\": mode_of_sentiment_score,\n",
        "        \"mode_of_avg_sentiment_score\": mode_of_avg_sentiment_score\n",
        "    })\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# 3. MAIN PROCESS\n",
        "# -----------------------\n",
        "TICKER_UNIVERSE = [\n",
        "    'AAPL', 'MSFT', 'NVDA', 'AVGO', 'ADBE', 'UNH', 'JNJ', 'PFE', 'MRK', 'ABBV',\n",
        "    'JPM', 'BAC', 'WFC', 'GS', 'MS', 'AMZN', 'TSLA', 'HD', 'MCD', 'NKE', 'GOOGL',\n",
        "    'META', 'DIS', 'VZ', 'CMCSA', 'PG', 'KO', 'PEP', 'WMT', 'COST', 'XOM', 'CVX',\n",
        "    'COP', 'BA', 'UNP', 'HON', 'NEE', 'DUK', 'SO', 'PLD', 'AMT', 'CCI', 'SHW', 'DOW'\n",
        "]\n",
        "\n",
        "# Directory containing all TICKER.parquet files\n",
        "input_dir = \"/content/drive/My Drive/LLM_RL_Data/stock_parquet\"\n",
        "output_dir = \"/content/drive/My Drive/LLM_RL_Data/llm_sentiments\"\n",
        "\n",
        "# Optional: warm-up pass to stabilize GPU memory\n",
        "print(\"Running warm-up pass...\")\n",
        "dummy_input = tokenizer(\"Warm-up\", return_tensors=\"pt\", padding=True, max_length=128).to(device)\n",
        "with torch.no_grad():\n",
        "    _ = model(**dummy_input)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "# Loop over all tickers\n",
        "for ticker in ['SO']:\n",
        "    input_path = f\"{input_dir}/{ticker}.parquet\"\n",
        "    try:\n",
        "        df = pd.read_parquet(input_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found for {ticker}, skipping...\")\n",
        "        continue\n",
        "\n",
        "    # The parquet file has a Date index (trading_day) and one column named exactly the same as the ticker.\n",
        "    # Each cell in that column is a list of articles (or None).\n",
        "    if ticker not in df.columns:\n",
        "        print(f\"Ticker column '{ticker}' not found in {input_path}, skipping...\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nProcessing {ticker} from file: {input_path}\")\n",
        "    print(f\"Data shape: {df.shape}\")\n",
        "\n",
        "    # Step 1: Convert the index into a column if needed\n",
        "    if df.index.name == \"trading_day\":\n",
        "        df.reset_index(inplace=True)\n",
        "\n",
        "    # Step 2: Explode the list of articles so each article is its own row\n",
        "    df_exploded = df.explode(ticker, ignore_index=False).reset_index(drop=True)\n",
        "    df_exploded.rename(columns={ticker: \"article_text\"}, inplace=True)\n",
        "\n",
        "    # If the parquet file can have None/NaN in the list, fill with empty strings\n",
        "    df_exploded[\"article_text\"] = df_exploded[\"article_text\"].fillna(\"\")\n",
        "\n",
        "    # Step 3: Apply get_sentiment to each article\n",
        "    #         This can be slow for large data; you might want batching if needed.\n",
        "    sentiments, logits = [], []\n",
        "    for text in tqdm(df_exploded[\"article_text\"], desc=f\"{ticker} articles\"):\n",
        "        s_label, s_logit = get_sentiment(text)\n",
        "        sentiments.append(s_label)\n",
        "        logits.append(s_logit)\n",
        "\n",
        "    df_exploded[\"sentiment\"] = sentiments\n",
        "    df_exploded[\"sentiment_logit\"] = logits\n",
        "\n",
        "    # Step 4: Group by original date to compute daily aggregates\n",
        "    # Make sure you still have a date column (trading_day)\n",
        "    # If it was originally the index, check how you want to name it in the exploded DataFrame.\n",
        "    # Suppose the original date column is \"trading_day\"\n",
        "    if \"trading_day\" not in df_exploded.columns:\n",
        "        # If needed, rename the old index name or re-check how the date is stored\n",
        "        pass\n",
        "\n",
        "    grouped_results = (\n",
        "        df_exploded\n",
        "        .groupby(\"trading_day\", as_index=True)\n",
        "        .apply(aggregate_daily_sentiment)\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # Step 5: Save the aggregated result for this ticker\n",
        "    ticker_output_path = f\"{output_dir}/{ticker}_sentiment_agg.parquet\"\n",
        "    grouped_results.to_parquet(ticker_output_path, index=False)\n",
        "\n",
        "    print(f\"Saved daily sentiment aggregates to {ticker_output_path}\")\n",
        "\n",
        "    # Optional memory cleanup\n",
        "    del df, df_exploded, grouped_results, sentiments, logits\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "print(\"\\nAll tickers processed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXLGze2fhWh1",
        "outputId": "bfc76497-0399-4614-a610-558f8709705e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running warm-up pass...\n",
            "\n",
            "Processing SO from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/SO.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SO articles: 100%|██████████| 2828/2828 [02:31<00:00, 18.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/SO_sentiment_agg.parquet\n",
            "\n",
            "All tickers processed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# -----------------------\n",
        "# 1. Industry Mapping Dictionary\n",
        "# -----------------------\n",
        "TICKER_INDUSTRY_MAP = {\n",
        "    # Technology\n",
        "    'AAPL': 'Technology',\n",
        "    'MSFT': 'Technology',\n",
        "    'NVDA': 'Technology',\n",
        "    'AVGO': 'Technology',\n",
        "    'ADBE': 'Technology',\n",
        "    'GOOGL': 'Technology',\n",
        "    'META': 'Technology',\n",
        "\n",
        "    # Healthcare\n",
        "    'UNH': 'Healthcare',\n",
        "    'JNJ': 'Healthcare',\n",
        "    'PFE': 'Healthcare',\n",
        "    'MRK': 'Healthcare',\n",
        "    'ABBV': 'Healthcare',\n",
        "\n",
        "    # Financials\n",
        "    'JPM': 'Financials',\n",
        "    'BAC': 'Financials',\n",
        "    'WFC': 'Financials',\n",
        "    'GS': 'Financials',\n",
        "    'MS': 'Financials',\n",
        "\n",
        "    # Consumer Discretionary\n",
        "    'AMZN': 'Consumer Discretionary',\n",
        "    'TSLA': 'Consumer Discretionary',\n",
        "    'HD': 'Consumer Discretionary',\n",
        "    'MCD': 'Consumer Discretionary',\n",
        "    'NKE': 'Consumer Discretionary',\n",
        "\n",
        "    # Communication Services\n",
        "    'DIS': 'Communication Services',\n",
        "    'VZ': 'Communication Services',\n",
        "    'CMCSA': 'Communication Services',\n",
        "\n",
        "    # Consumer Staples\n",
        "    'PG': 'Consumer Staples',\n",
        "    'KO': 'Consumer Staples',\n",
        "    'PEP': 'Consumer Staples',\n",
        "    'WMT': 'Consumer Staples',\n",
        "    'COST': 'Consumer Staples',\n",
        "\n",
        "    # Energy\n",
        "    'XOM': 'Energy',\n",
        "    'CVX': 'Energy',\n",
        "    'COP': 'Energy',\n",
        "\n",
        "    # Industrials / Materials\n",
        "    'BA': 'Industrials',\n",
        "    'UNP': 'Industrials',\n",
        "    'HON': 'Industrials',\n",
        "    'SHW': 'Industrials',\n",
        "    'DOW': 'Materials',\n",
        "\n",
        "    # Utilities\n",
        "    'NEE': 'Utilities',\n",
        "    'DUK': 'Utilities',\n",
        "    'SO': 'Utilities',\n",
        "\n",
        "    # Real Estate\n",
        "    'PLD': 'Real Estate',\n",
        "    'AMT': 'Real Estate',\n",
        "    'CCI': 'Real Estate'\n",
        "}\n",
        "\n",
        "# -----------------------\n",
        "# 2. PREP: MODEL + TOKENIZER\n",
        "# -----------------------\n",
        "# Ensure you've defined/imported:\n",
        "#   tokenizer, model, device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Updated get_sentiment function to accept an industry argument\n",
        "def get_sentiment(text, industry=None):\n",
        "    \"\"\"Return (sentiment_label, sentiment_probability) with optional industry context.\"\"\"\n",
        "    if not text:\n",
        "        # If empty text, treat as neutral\n",
        "        return \"Neutral\", 1.0\n",
        "\n",
        "    # (Optional) Truncate very long text to avoid GPU OOM\n",
        "    len_text = min(len(text), 5000)\n",
        "    text = text[:len_text]\n",
        "\n",
        "    # Prepend industry information if provided\n",
        "    industry_text = f\"Industry: {industry}.\\n\" if industry is not None else \"\"\n",
        "    prompt = (\n",
        "        f\"{industry_text}\"\n",
        "        f\"Instruction: What is the sentiment of this news? \"\n",
        "        f\"Please choose an answer from [Positive, Negative, Neutral].\\n\"\n",
        "        f\"Input: {text}\\nAnswer: \"\n",
        "    )\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, max_length=128).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Extract logits for the last token\n",
        "    logits = outputs.logits[:, -1, :].to(\"cpu\")\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    # Map the probabilities to the sentiment tokens\n",
        "    class_tokens = tokenizer([\"Positive\", \"Negative\", \"Neutral\"], add_special_tokens=False)[\"input_ids\"]\n",
        "    class_probs = {tokenizer.decode(token_id): probs[0, token_id].item() for token_id in class_tokens}\n",
        "\n",
        "    # Pick the highest-probability sentiment\n",
        "    sentiment_label = max(class_probs, key=class_probs.get)\n",
        "    sentiment_prob = class_probs[sentiment_label]\n",
        "\n",
        "    # Clean up\n",
        "    del inputs, outputs, logits, probs\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return sentiment_label, sentiment_prob\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# 3. AGGREGATION FUNCTION\n",
        "# -----------------------\n",
        "def aggregate_daily_sentiment(group: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Given a subset of rows (one day of articles),\n",
        "    compute the required metrics, including combined sentiment.\n",
        "    \"\"\"\n",
        "    total_articles = len(group)\n",
        "    if total_articles == 0:\n",
        "        return pd.Series({\n",
        "            \"mode_of_sentiment\": None,\n",
        "            \"num_articles\": 0,\n",
        "            \"ratio_positive\": 0.0,\n",
        "            \"ratio_negative\": 0.0,\n",
        "            \"ratio_neutral\": 0.0,\n",
        "            \"avg_sentiment_positive\": 0.0,\n",
        "            \"avg_sentiment_negative\": 0.0,\n",
        "            \"avg_sentiment_neutral\": 0.0,\n",
        "            \"weighted_avg_sentiment\": 0.0,\n",
        "            \"mode_of_sentiment_score\": None,\n",
        "            \"mode_of_avg_sentiment_score\": None,\n",
        "            \"sentiment_combined\": None,\n",
        "            \"sentiment_score_combined\": 0.0\n",
        "        })\n",
        "\n",
        "    # 1) Mode of sentiment types (which label appears most)\n",
        "    sentiment_counts = group[\"sentiment\"].value_counts()\n",
        "    mode_of_sentiment = sentiment_counts.idxmax()\n",
        "\n",
        "    # 2) Number of articles\n",
        "    num_articles = total_articles\n",
        "\n",
        "    # 3) Ratio of positive/negative/neutral\n",
        "    ratio_positive = sentiment_counts.get(\"Positive\", 0) / total_articles\n",
        "    ratio_negative = sentiment_counts.get(\"Negative\", 0) / total_articles\n",
        "    ratio_neutral  = sentiment_counts.get(\"Neutral\",  0) / total_articles\n",
        "\n",
        "    # 4) Average sentiment for positive, negative, neutral\n",
        "    avg_sentiment_positive = group.loc[group[\"sentiment\"] == \"Positive\", \"sentiment_logit\"].mean()\n",
        "    avg_sentiment_negative = group.loc[group[\"sentiment\"] == \"Negative\", \"sentiment_logit\"].mean()\n",
        "    avg_sentiment_neutral  = group.loc[group[\"sentiment\"] == \"Neutral\",  \"sentiment_logit\"].mean()\n",
        "\n",
        "    # 5) Weighted average sentiment:\n",
        "    #    (sum of all positive sentiments - sum of all negative sentiments) / total_articles\n",
        "    sum_pos = group.loc[group[\"sentiment\"] == \"Positive\", \"sentiment_logit\"].sum()\n",
        "    sum_neg = group.loc[group[\"sentiment\"] == \"Negative\", \"sentiment_logit\"].sum()\n",
        "    weighted_avg_sentiment = (sum_pos - sum_neg) / total_articles\n",
        "\n",
        "    # 6) Mode of sentiment score (which sentiment label has the highest sum of probabilities)\n",
        "    sum_sentiment_scores = group.groupby(\"sentiment\")[\"sentiment_logit\"].sum()\n",
        "    mode_of_sentiment_score = sum_sentiment_scores.idxmax() if not sum_sentiment_scores.empty else None\n",
        "\n",
        "    # 7) Mode of average sentiment score (which label has the highest average probability)\n",
        "    avg_sentiment_scores = group.groupby(\"sentiment\")[\"sentiment_logit\"].mean()\n",
        "    mode_of_avg_sentiment_score = avg_sentiment_scores.idxmax() if not avg_sentiment_scores.empty else None\n",
        "\n",
        "    # 8) Combined sentiment for all articles of the day:\n",
        "    combined_text = \" \".join(group[\"article_text\"].tolist()).strip()\n",
        "    # Retrieve industry from the group (all rows have the same ticker, hence same industry)\n",
        "    industry = group[\"industry\"].iloc[0] if \"industry\" in group.columns else None\n",
        "    sentiment_combined, sentiment_score_combined = get_sentiment(combined_text, industry=industry)\n",
        "\n",
        "    return pd.Series({\n",
        "        \"mode_of_sentiment\": mode_of_sentiment,\n",
        "        \"num_articles\": num_articles,\n",
        "        \"ratio_positive\": ratio_positive,\n",
        "        \"ratio_negative\": ratio_negative,\n",
        "        \"ratio_neutral\":  ratio_neutral,\n",
        "        \"avg_sentiment_positive\": avg_sentiment_positive,\n",
        "        \"avg_sentiment_negative\": avg_sentiment_negative,\n",
        "        \"avg_sentiment_neutral\":  avg_sentiment_neutral,\n",
        "        \"weighted_avg_sentiment\": weighted_avg_sentiment,\n",
        "        \"mode_of_sentiment_score\": mode_of_sentiment_score,\n",
        "        \"mode_of_avg_sentiment_score\": mode_of_avg_sentiment_score,\n",
        "        \"sentiment_combined\": sentiment_combined,\n",
        "        \"sentiment_score_combined\": sentiment_score_combined\n",
        "    })\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# 4. MAIN PROCESS\n",
        "# -----------------------\n",
        "TICKER_UNIVERSE = [\n",
        "    'AAPL', 'MSFT', 'NVDA', 'AVGO', 'ADBE', 'UNH', 'JNJ', 'PFE', 'MRK', 'ABBV',\n",
        "    # 'JPM', 'BAC', 'WFC', 'GS', 'MS', 'AMZN', 'TSLA', 'HD', 'MCD', 'NKE', 'GOOGL',\n",
        "    # 'META', 'DIS', 'VZ', 'CMCSA', 'PG', 'KO', 'PEP', 'WMT', 'COST', 'XOM', 'CVX',\n",
        "    # 'COP', 'BA', 'UNP', 'HON', 'NEE', 'DUK', 'SO', 'PLD', 'AMT', 'CCI', 'SHW', 'DOW'\n",
        "]\n",
        "\n",
        "# Directory containing all TICKER.parquet files\n",
        "input_dir = \"/content/drive/My Drive/LLM_RL_Data/stock_parquet\"\n",
        "output_dir = \"/content/drive/My Drive/LLM_RL_Data/llm_sentiments\"\n",
        "\n",
        "# Optional: warm-up pass to stabilize GPU memory\n",
        "print(\"Running warm-up pass...\")\n",
        "dummy_input = tokenizer(\"Warm-up\", return_tensors=\"pt\", padding=True, max_length=128).to(device)\n",
        "with torch.no_grad():\n",
        "    _ = model(**dummy_input)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Loop over tickers (example with ticker 'SO')\n",
        "for ticker in TICKER_UNIVERSE:\n",
        "    input_path = f\"{input_dir}/{ticker}.parquet\"\n",
        "    try:\n",
        "        df = pd.read_parquet(input_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found for {ticker}, skipping...\")\n",
        "        continue\n",
        "\n",
        "    if ticker not in df.columns:\n",
        "        print(f\"Ticker column '{ticker}' not found in {input_path}, skipping...\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nProcessing {ticker} from file: {input_path}\")\n",
        "    print(f\"Data shape: {df.shape}\")\n",
        "\n",
        "    # Convert index to column if needed\n",
        "    if df.index.name == \"trading_day\":\n",
        "        df.reset_index(inplace=True)\n",
        "\n",
        "    # Explode the list of articles so each article is its own row\n",
        "    df_exploded = df.explode(ticker, ignore_index=False).reset_index(drop=True)\n",
        "    df_exploded.rename(columns={ticker: \"article_text\"}, inplace=True)\n",
        "\n",
        "    # Fill missing article text with empty strings\n",
        "    df_exploded[\"article_text\"] = df_exploded[\"article_text\"].fillna(\"\")\n",
        "\n",
        "    # Add a column for industry using the mapping\n",
        "    industry = TICKER_INDUSTRY_MAP.get(ticker, None)\n",
        "    df_exploded[\"industry\"] = industry\n",
        "\n",
        "    # Apply get_sentiment to each article\n",
        "    sentiments, logits = [], []\n",
        "    for text in tqdm(df_exploded[\"article_text\"], desc=f\"{ticker} articles\"):\n",
        "        s_label, s_logit = get_sentiment(text, industry=industry)\n",
        "        sentiments.append(s_label)\n",
        "        logits.append(s_logit)\n",
        "\n",
        "    df_exploded[\"sentiment\"] = sentiments\n",
        "    df_exploded[\"sentiment_logit\"] = logits\n",
        "\n",
        "    # Group by trading_day to compute daily aggregates\n",
        "    grouped_results = (\n",
        "        df_exploded\n",
        "        .groupby(\"trading_day\", as_index=True)\n",
        "        .apply(aggregate_daily_sentiment)\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # Save the aggregated result for this ticker\n",
        "    ticker_output_path = f\"{output_dir}/{ticker}_sentiment_agg.parquet\"\n",
        "    grouped_results.to_parquet(ticker_output_path, index=False)\n",
        "\n",
        "    print(f\"Saved daily sentiment aggregates to {ticker_output_path}\")\n",
        "\n",
        "    # Cleanup\n",
        "    del df, df_exploded, grouped_results, sentiments, logits\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "print(\"\\nAll tickers processed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYRDL1RNsxfS",
        "outputId": "7a4c327d-a5e8-4b60-de4b-fbc23a9d5b7e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running warm-up pass...\n",
            "\n",
            "Processing AAPL from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/AAPL.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AAPL articles: 100%|██████████| 27318/27318 [41:21<00:00, 11.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/AAPL_sentiment_agg.parquet\n",
            "\n",
            "Processing MSFT from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/MSFT.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MSFT articles: 100%|██████████| 16769/16769 [24:14<00:00, 11.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/MSFT_sentiment_agg.parquet\n",
            "\n",
            "Processing NVDA from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/NVDA.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "NVDA articles: 100%|██████████| 10918/10918 [16:18<00:00, 11.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/NVDA_sentiment_agg.parquet\n",
            "\n",
            "Processing AVGO from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/AVGO.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVGO articles: 100%|██████████| 4289/4289 [05:27<00:00, 13.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/AVGO_sentiment_agg.parquet\n",
            "\n",
            "Processing ADBE from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/ADBE.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ADBE articles: 100%|██████████| 3366/3366 [03:39<00:00, 15.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/ADBE_sentiment_agg.parquet\n",
            "\n",
            "Processing UNH from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/UNH.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UNH articles: 100%|██████████| 4304/4304 [05:12<00:00, 13.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/UNH_sentiment_agg.parquet\n",
            "\n",
            "Processing JNJ from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/JNJ.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "JNJ articles: 100%|██████████| 10630/10630 [14:29<00:00, 12.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/JNJ_sentiment_agg.parquet\n",
            "\n",
            "Processing PFE from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/PFE.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PFE articles: 100%|██████████| 19229/19229 [26:52<00:00, 11.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/PFE_sentiment_agg.parquet\n",
            "\n",
            "Processing MRK from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/MRK.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MRK articles: 100%|██████████| 6420/6420 [08:25<00:00, 12.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/MRK_sentiment_agg.parquet\n",
            "\n",
            "Processing ABBV from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/ABBV.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ABBV articles: 100%|██████████| 4447/4447 [05:50<00:00, 12.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/ABBV_sentiment_agg.parquet\n",
            "\n",
            "All tickers processed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# -----------------------\n",
        "# 1. Industry Mapping Dictionary\n",
        "# -----------------------\n",
        "TICKER_INDUSTRY_MAP = {\n",
        "    # Technology\n",
        "    'AAPL': 'Technology',\n",
        "    'MSFT': 'Technology',\n",
        "    'NVDA': 'Technology',\n",
        "    'AVGO': 'Technology',\n",
        "    'ADBE': 'Technology',\n",
        "    'GOOGL': 'Technology',\n",
        "    'META': 'Technology',\n",
        "\n",
        "    # Healthcare\n",
        "    'UNH': 'Healthcare',\n",
        "    'JNJ': 'Healthcare',\n",
        "    'PFE': 'Healthcare',\n",
        "    'MRK': 'Healthcare',\n",
        "    'ABBV': 'Healthcare',\n",
        "\n",
        "    # Financials\n",
        "    'JPM': 'Financials',\n",
        "    'BAC': 'Financials',\n",
        "    'WFC': 'Financials',\n",
        "    'GS': 'Financials',\n",
        "    'MS': 'Financials',\n",
        "\n",
        "    # Consumer Discretionary\n",
        "    'AMZN': 'Consumer Discretionary',\n",
        "    'TSLA': 'Consumer Discretionary',\n",
        "    'HD': 'Consumer Discretionary',\n",
        "    'MCD': 'Consumer Discretionary',\n",
        "    'NKE': 'Consumer Discretionary',\n",
        "\n",
        "    # Communication Services\n",
        "    'DIS': 'Communication Services',\n",
        "    'VZ': 'Communication Services',\n",
        "    'CMCSA': 'Communication Services',\n",
        "\n",
        "    # Consumer Staples\n",
        "    'PG': 'Consumer Staples',\n",
        "    'KO': 'Consumer Staples',\n",
        "    'PEP': 'Consumer Staples',\n",
        "    'WMT': 'Consumer Staples',\n",
        "    'COST': 'Consumer Staples',\n",
        "\n",
        "    # Energy\n",
        "    'XOM': 'Energy',\n",
        "    'CVX': 'Energy',\n",
        "    'COP': 'Energy',\n",
        "\n",
        "    # Industrials / Materials\n",
        "    'BA': 'Industrials',\n",
        "    'UNP': 'Industrials',\n",
        "    'HON': 'Industrials',\n",
        "    'SHW': 'Industrials',\n",
        "    'DOW': 'Materials',\n",
        "\n",
        "    # Utilities\n",
        "    'NEE': 'Utilities',\n",
        "    'DUK': 'Utilities',\n",
        "    'SO': 'Utilities',\n",
        "\n",
        "    # Real Estate\n",
        "    'PLD': 'Real Estate',\n",
        "    'AMT': 'Real Estate',\n",
        "    'CCI': 'Real Estate'\n",
        "}\n",
        "\n",
        "# -----------------------\n",
        "# 2. PREP: MODEL + TOKENIZER\n",
        "# -----------------------\n",
        "# Ensure you've defined/imported:\n",
        "#   tokenizer, model, device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Updated get_sentiment function to accept an industry argument\n",
        "def get_sentiment(text, industry=None):\n",
        "    \"\"\"Return (sentiment_label, sentiment_probability) with optional industry context.\"\"\"\n",
        "    if not text:\n",
        "        # If empty text, treat as neutral\n",
        "        return \"Neutral\", 1.0\n",
        "\n",
        "    # (Optional) Truncate very long text to avoid GPU OOM\n",
        "    len_text = min(len(text), 5000)\n",
        "    text = text[:len_text]\n",
        "\n",
        "    # Prepend industry information if provided\n",
        "    industry_text = f\"Industry: {industry}.\\n\" if industry is not None else \"\"\n",
        "    prompt = (\n",
        "        f\"{industry_text}\"\n",
        "        f\"Instruction: What is the sentiment of this news? \"\n",
        "        f\"Please choose an answer from [Positive, Negative, Neutral].\\n\"\n",
        "        f\"Input: {text}\\nAnswer: \"\n",
        "    )\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, max_length=128).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Extract logits for the last token\n",
        "    logits = outputs.logits[:, -1, :].to(\"cpu\")\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    # Map the probabilities to the sentiment tokens\n",
        "    class_tokens = tokenizer([\"Positive\", \"Negative\", \"Neutral\"], add_special_tokens=False)[\"input_ids\"]\n",
        "    class_probs = {tokenizer.decode(token_id): probs[0, token_id].item() for token_id in class_tokens}\n",
        "\n",
        "    # Pick the highest-probability sentiment\n",
        "    sentiment_label = max(class_probs, key=class_probs.get)\n",
        "    sentiment_prob = class_probs[sentiment_label]\n",
        "\n",
        "    # Clean up\n",
        "    del inputs, outputs, logits, probs\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return sentiment_label, sentiment_prob\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# 3. AGGREGATION FUNCTION\n",
        "# -----------------------\n",
        "def aggregate_daily_sentiment(group: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Given a subset of rows (one day of articles),\n",
        "    compute the required metrics, including combined sentiment.\n",
        "    \"\"\"\n",
        "    total_articles = len(group)\n",
        "    if total_articles == 0:\n",
        "        return pd.Series({\n",
        "            \"mode_of_sentiment\": None,\n",
        "            \"num_articles\": 0,\n",
        "            \"ratio_positive\": 0.0,\n",
        "            \"ratio_negative\": 0.0,\n",
        "            \"ratio_neutral\": 0.0,\n",
        "            \"avg_sentiment_positive\": 0.0,\n",
        "            \"avg_sentiment_negative\": 0.0,\n",
        "            \"avg_sentiment_neutral\": 0.0,\n",
        "            \"weighted_avg_sentiment\": 0.0,\n",
        "            \"mode_of_sentiment_score\": None,\n",
        "            \"mode_of_avg_sentiment_score\": None,\n",
        "            \"sentiment_combined\": None,\n",
        "            \"sentiment_score_combined\": 0.0\n",
        "        })\n",
        "\n",
        "    # 1) Mode of sentiment types (which label appears most)\n",
        "    sentiment_counts = group[\"sentiment\"].value_counts()\n",
        "    mode_of_sentiment = sentiment_counts.idxmax()\n",
        "\n",
        "    # 2) Number of articles\n",
        "    num_articles = total_articles\n",
        "\n",
        "    # 3) Ratio of positive/negative/neutral\n",
        "    ratio_positive = sentiment_counts.get(\"Positive\", 0) / total_articles\n",
        "    ratio_negative = sentiment_counts.get(\"Negative\", 0) / total_articles\n",
        "    ratio_neutral  = sentiment_counts.get(\"Neutral\",  0) / total_articles\n",
        "\n",
        "    # 4) Average sentiment for positive, negative, neutral\n",
        "    avg_sentiment_positive = group.loc[group[\"sentiment\"] == \"Positive\", \"sentiment_logit\"].mean()\n",
        "    avg_sentiment_negative = group.loc[group[\"sentiment\"] == \"Negative\", \"sentiment_logit\"].mean()\n",
        "    avg_sentiment_neutral  = group.loc[group[\"sentiment\"] == \"Neutral\",  \"sentiment_logit\"].mean()\n",
        "\n",
        "    # 5) Weighted average sentiment:\n",
        "    #    (sum of all positive sentiments - sum of all negative sentiments) / total_articles\n",
        "    sum_pos = group.loc[group[\"sentiment\"] == \"Positive\", \"sentiment_logit\"].sum()\n",
        "    sum_neg = group.loc[group[\"sentiment\"] == \"Negative\", \"sentiment_logit\"].sum()\n",
        "    weighted_avg_sentiment = (sum_pos - sum_neg) / total_articles\n",
        "\n",
        "    # 6) Mode of sentiment score (which sentiment label has the highest sum of probabilities)\n",
        "    sum_sentiment_scores = group.groupby(\"sentiment\")[\"sentiment_logit\"].sum()\n",
        "    mode_of_sentiment_score = sum_sentiment_scores.idxmax() if not sum_sentiment_scores.empty else None\n",
        "\n",
        "    # 7) Mode of average sentiment score (which label has the highest average probability)\n",
        "    avg_sentiment_scores = group.groupby(\"sentiment\")[\"sentiment_logit\"].mean()\n",
        "    mode_of_avg_sentiment_score = avg_sentiment_scores.idxmax() if not avg_sentiment_scores.empty else None\n",
        "\n",
        "    # 8) Combined sentiment for all articles of the day:\n",
        "    combined_text = \" \".join(group[\"article_text\"].tolist()).strip()\n",
        "    # Retrieve industry from the group (all rows have the same ticker, hence same industry)\n",
        "    industry = group[\"industry\"].iloc[0] if \"industry\" in group.columns else None\n",
        "    sentiment_combined, sentiment_score_combined = get_sentiment(combined_text, industry=industry)\n",
        "\n",
        "    return pd.Series({\n",
        "        \"mode_of_sentiment\": mode_of_sentiment,\n",
        "        \"num_articles\": num_articles,\n",
        "        \"ratio_positive\": ratio_positive,\n",
        "        \"ratio_negative\": ratio_negative,\n",
        "        \"ratio_neutral\":  ratio_neutral,\n",
        "        \"avg_sentiment_positive\": avg_sentiment_positive,\n",
        "        \"avg_sentiment_negative\": avg_sentiment_negative,\n",
        "        \"avg_sentiment_neutral\":  avg_sentiment_neutral,\n",
        "        \"weighted_avg_sentiment\": weighted_avg_sentiment,\n",
        "        \"mode_of_sentiment_score\": mode_of_sentiment_score,\n",
        "        \"mode_of_avg_sentiment_score\": mode_of_avg_sentiment_score,\n",
        "        \"sentiment_combined\": sentiment_combined,\n",
        "        \"sentiment_score_combined\": sentiment_score_combined\n",
        "    })\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# 4. MAIN PROCESS\n",
        "# -----------------------\n",
        "TICKER_UNIVERSE = [\n",
        "    # 'AAPL', 'MSFT', 'NVDA', 'AVGO', 'ADBE', 'UNH', 'JNJ', 'PFE', 'MRK', 'ABBV',\n",
        "    'JPM', 'BAC', 'WFC', 'GS', 'MS', 'AMZN', 'TSLA', 'HD', 'MCD', 'NKE', 'GOOGL',\n",
        "    'META', 'DIS', 'VZ', 'CMCSA', 'PG', 'KO', 'PEP', 'WMT', 'COST', 'XOM', 'CVX',\n",
        "    # 'COP', 'BA', 'UNP', 'HON', 'NEE', 'DUK', 'SO', 'PLD', 'AMT', 'CCI', 'SHW', 'DOW'\n",
        "]\n",
        "\n",
        "# Directory containing all TICKER.parquet files\n",
        "input_dir = \"/content/drive/My Drive/LLM_RL_Data/stock_parquet\"\n",
        "output_dir = \"/content/drive/My Drive/LLM_RL_Data/llm_sentiments\"\n",
        "\n",
        "# Optional: warm-up pass to stabilize GPU memory\n",
        "print(\"Running warm-up pass...\")\n",
        "dummy_input = tokenizer(\"Warm-up\", return_tensors=\"pt\", padding=True, max_length=128).to(device)\n",
        "with torch.no_grad():\n",
        "    _ = model(**dummy_input)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Loop over tickers (example with ticker 'SO')\n",
        "for ticker in TICKER_UNIVERSE:\n",
        "    input_path = f\"{input_dir}/{ticker}.parquet\"\n",
        "    try:\n",
        "        df = pd.read_parquet(input_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found for {ticker}, skipping...\")\n",
        "        continue\n",
        "\n",
        "    if ticker not in df.columns:\n",
        "        print(f\"Ticker column '{ticker}' not found in {input_path}, skipping...\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nProcessing {ticker} from file: {input_path}\")\n",
        "    print(f\"Data shape: {df.shape}\")\n",
        "\n",
        "    # Convert index to column if needed\n",
        "    if df.index.name == \"trading_day\":\n",
        "        df.reset_index(inplace=True)\n",
        "\n",
        "    # Explode the list of articles so each article is its own row\n",
        "    df_exploded = df.explode(ticker, ignore_index=False).reset_index(drop=True)\n",
        "    df_exploded.rename(columns={ticker: \"article_text\"}, inplace=True)\n",
        "\n",
        "    # Fill missing article text with empty strings\n",
        "    df_exploded[\"article_text\"] = df_exploded[\"article_text\"].fillna(\"\")\n",
        "\n",
        "    # Add a column for industry using the mapping\n",
        "    industry = TICKER_INDUSTRY_MAP.get(ticker, None)\n",
        "    df_exploded[\"industry\"] = industry\n",
        "\n",
        "    # Apply get_sentiment to each article\n",
        "    sentiments, logits = [], []\n",
        "    for text in tqdm(df_exploded[\"article_text\"], desc=f\"{ticker} articles\"):\n",
        "        s_label, s_logit = get_sentiment(text, industry=industry)\n",
        "        sentiments.append(s_label)\n",
        "        logits.append(s_logit)\n",
        "\n",
        "    df_exploded[\"sentiment\"] = sentiments\n",
        "    df_exploded[\"sentiment_logit\"] = logits\n",
        "\n",
        "    # Group by trading_day to compute daily aggregates\n",
        "    grouped_results = (\n",
        "        df_exploded\n",
        "        .groupby(\"trading_day\", as_index=True)\n",
        "        .apply(aggregate_daily_sentiment)\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # Save the aggregated result for this ticker\n",
        "    ticker_output_path = f\"{output_dir}/{ticker}_sentiment_agg.parquet\"\n",
        "    grouped_results.to_parquet(ticker_output_path, index=False)\n",
        "\n",
        "    print(f\"Saved daily sentiment aggregates to {ticker_output_path}\")\n",
        "\n",
        "    # Cleanup\n",
        "    del df, df_exploded, grouped_results, sentiments, logits\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "print(\"\\nAll tickers processed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1kutz4kiXlc",
        "outputId": "1dbb4a41-39e0-4ecf-d029-7304b995ed8f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running warm-up pass...\n",
            "\n",
            "Processing JPM from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/JPM.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "JPM articles: 100%|██████████| 15768/15768 [22:51<00:00, 11.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/JPM_sentiment_agg.parquet\n",
            "\n",
            "Processing BAC from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/BAC.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BAC articles: 100%|██████████| 9705/9705 [14:30<00:00, 11.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/BAC_sentiment_agg.parquet\n",
            "\n",
            "Processing WFC from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/WFC.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WFC articles: 100%|██████████| 7548/7548 [10:40<00:00, 11.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/WFC_sentiment_agg.parquet\n",
            "\n",
            "Processing GS from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/GS.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GS articles: 100%|██████████| 14701/14701 [21:26<00:00, 11.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/GS_sentiment_agg.parquet\n",
            "\n",
            "Processing MS from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/MS.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MS articles: 100%|██████████| 10698/10698 [15:53<00:00, 11.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/MS_sentiment_agg.parquet\n",
            "\n",
            "Processing AMZN from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/AMZN.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AMZN articles: 100%|██████████| 27470/27470 [40:08<00:00, 11.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/AMZN_sentiment_agg.parquet\n",
            "\n",
            "Processing TSLA from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/TSLA.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TSLA articles: 100%|██████████| 27808/27808 [41:46<00:00, 11.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/TSLA_sentiment_agg.parquet\n",
            "\n",
            "Processing HD from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/HD.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "HD articles: 100%|██████████| 3891/3891 [04:40<00:00, 13.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/HD_sentiment_agg.parquet\n",
            "\n",
            "Processing MCD from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/MCD.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MCD articles: 100%|██████████| 4702/4702 [06:02<00:00, 12.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/MCD_sentiment_agg.parquet\n",
            "\n",
            "Processing NKE from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/NKE.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "NKE articles: 100%|██████████| 5392/5392 [07:15<00:00, 12.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/NKE_sentiment_agg.parquet\n",
            "\n",
            "Processing GOOGL from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/GOOGL.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "GOOGL articles: 100%|██████████| 23108/23108 [33:34<00:00, 11.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/GOOGL_sentiment_agg.parquet\n",
            "\n",
            "Processing META from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/META.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "META articles: 100%|██████████| 8020/8020 [10:00<00:00, 13.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/META_sentiment_agg.parquet\n",
            "\n",
            "Processing DIS from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/DIS.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DIS articles: 100%|██████████| 9391/9391 [12:52<00:00, 12.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/DIS_sentiment_agg.parquet\n",
            "\n",
            "Processing VZ from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/VZ.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "VZ articles: 100%|██████████| 4679/4679 [05:36<00:00, 13.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/VZ_sentiment_agg.parquet\n",
            "\n",
            "Processing CMCSA from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/CMCSA.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CMCSA articles: 100%|██████████| 5055/5055 [06:35<00:00, 12.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/CMCSA_sentiment_agg.parquet\n",
            "\n",
            "Processing PG from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/PG.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PG articles: 100%|██████████| 4071/4071 [04:58<00:00, 13.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/PG_sentiment_agg.parquet\n",
            "\n",
            "Processing KO from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/KO.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "KO articles: 100%|██████████| 4565/4565 [05:57<00:00, 12.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/KO_sentiment_agg.parquet\n",
            "\n",
            "Processing PEP from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/PEP.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PEP articles: 100%|██████████| 3352/3352 [03:22<00:00, 16.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/PEP_sentiment_agg.parquet\n",
            "\n",
            "Processing WMT from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/WMT.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WMT articles: 100%|██████████| 10535/10535 [14:53<00:00, 11.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/WMT_sentiment_agg.parquet\n",
            "\n",
            "Processing COST from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/COST.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "COST articles: 100%|██████████| 3404/3404 [03:51<00:00, 14.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/COST_sentiment_agg.parquet\n",
            "\n",
            "Processing XOM from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/XOM.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "XOM articles: 100%|██████████| 15253/15253 [22:36<00:00, 11.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/XOM_sentiment_agg.parquet\n",
            "\n",
            "Processing CVX from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/CVX.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CVX articles: 100%|██████████| 12602/12602 [19:27<00:00, 10.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/CVX_sentiment_agg.parquet\n",
            "\n",
            "All tickers processed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# -----------------------\n",
        "# 1. Industry Mapping Dictionary\n",
        "# -----------------------\n",
        "TICKER_INDUSTRY_MAP = {\n",
        "    # Technology\n",
        "    'AAPL': 'Technology',\n",
        "    'MSFT': 'Technology',\n",
        "    'NVDA': 'Technology',\n",
        "    'AVGO': 'Technology',\n",
        "    'ADBE': 'Technology',\n",
        "    'GOOGL': 'Technology',\n",
        "    'META': 'Technology',\n",
        "\n",
        "    # Healthcare\n",
        "    'UNH': 'Healthcare',\n",
        "    'JNJ': 'Healthcare',\n",
        "    'PFE': 'Healthcare',\n",
        "    'MRK': 'Healthcare',\n",
        "    'ABBV': 'Healthcare',\n",
        "\n",
        "    # Financials\n",
        "    'JPM': 'Financials',\n",
        "    'BAC': 'Financials',\n",
        "    'WFC': 'Financials',\n",
        "    'GS': 'Financials',\n",
        "    'MS': 'Financials',\n",
        "\n",
        "    # Consumer Discretionary\n",
        "    'AMZN': 'Consumer Discretionary',\n",
        "    'TSLA': 'Consumer Discretionary',\n",
        "    'HD': 'Consumer Discretionary',\n",
        "    'MCD': 'Consumer Discretionary',\n",
        "    'NKE': 'Consumer Discretionary',\n",
        "\n",
        "    # Communication Services\n",
        "    'DIS': 'Communication Services',\n",
        "    'VZ': 'Communication Services',\n",
        "    'CMCSA': 'Communication Services',\n",
        "\n",
        "    # Consumer Staples\n",
        "    'PG': 'Consumer Staples',\n",
        "    'KO': 'Consumer Staples',\n",
        "    'PEP': 'Consumer Staples',\n",
        "    'WMT': 'Consumer Staples',\n",
        "    'COST': 'Consumer Staples',\n",
        "\n",
        "    # Energy\n",
        "    'XOM': 'Energy',\n",
        "    'CVX': 'Energy',\n",
        "    'COP': 'Energy',\n",
        "\n",
        "    # Industrials / Materials\n",
        "    'BA': 'Industrials',\n",
        "    'UNP': 'Industrials',\n",
        "    'HON': 'Industrials',\n",
        "    'SHW': 'Industrials',\n",
        "    'DOW': 'Materials',\n",
        "\n",
        "    # Utilities\n",
        "    'NEE': 'Utilities',\n",
        "    'DUK': 'Utilities',\n",
        "    'SO': 'Utilities',\n",
        "\n",
        "    # Real Estate\n",
        "    'PLD': 'Real Estate',\n",
        "    'AMT': 'Real Estate',\n",
        "    'CCI': 'Real Estate'\n",
        "}\n",
        "\n",
        "# -----------------------\n",
        "# 2. PREP: MODEL + TOKENIZER\n",
        "# -----------------------\n",
        "# Ensure you've defined/imported:\n",
        "#   tokenizer, model, device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Updated get_sentiment function to accept an industry argument\n",
        "def get_sentiment(text, industry=None):\n",
        "    \"\"\"Return (sentiment_label, sentiment_probability) with optional industry context.\"\"\"\n",
        "    if not text:\n",
        "        # If empty text, treat as neutral\n",
        "        return \"Neutral\", 1.0\n",
        "\n",
        "    # (Optional) Truncate very long text to avoid GPU OOM\n",
        "    len_text = min(len(text), 5000)\n",
        "    text = text[:len_text]\n",
        "\n",
        "    # Prepend industry information if provided\n",
        "    industry_text = f\"Industry: {industry}.\\n\" if industry is not None else \"\"\n",
        "    prompt = (\n",
        "        f\"{industry_text}\"\n",
        "        f\"Instruction: What is the sentiment of this news? \"\n",
        "        f\"Please choose an answer from [Positive, Negative, Neutral].\\n\"\n",
        "        f\"Input: {text}\\nAnswer: \"\n",
        "    )\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, max_length=128).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Extract logits for the last token\n",
        "    logits = outputs.logits[:, -1, :].to(\"cpu\")\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    # Map the probabilities to the sentiment tokens\n",
        "    class_tokens = tokenizer([\"Positive\", \"Negative\", \"Neutral\"], add_special_tokens=False)[\"input_ids\"]\n",
        "    class_probs = {tokenizer.decode(token_id): probs[0, token_id].item() for token_id in class_tokens}\n",
        "\n",
        "    # Pick the highest-probability sentiment\n",
        "    sentiment_label = max(class_probs, key=class_probs.get)\n",
        "    sentiment_prob = class_probs[sentiment_label]\n",
        "\n",
        "    # Clean up\n",
        "    del inputs, outputs, logits, probs\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return sentiment_label, sentiment_prob\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# 3. AGGREGATION FUNCTION\n",
        "# -----------------------\n",
        "def aggregate_daily_sentiment(group: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Given a subset of rows (one day of articles),\n",
        "    compute the required metrics, including combined sentiment.\n",
        "    \"\"\"\n",
        "    total_articles = len(group)\n",
        "    if total_articles == 0:\n",
        "        return pd.Series({\n",
        "            \"mode_of_sentiment\": None,\n",
        "            \"num_articles\": 0,\n",
        "            \"ratio_positive\": 0.0,\n",
        "            \"ratio_negative\": 0.0,\n",
        "            \"ratio_neutral\": 0.0,\n",
        "            \"avg_sentiment_positive\": 0.0,\n",
        "            \"avg_sentiment_negative\": 0.0,\n",
        "            \"avg_sentiment_neutral\": 0.0,\n",
        "            \"weighted_avg_sentiment\": 0.0,\n",
        "            \"mode_of_sentiment_score\": None,\n",
        "            \"mode_of_avg_sentiment_score\": None,\n",
        "            \"sentiment_combined\": None,\n",
        "            \"sentiment_score_combined\": 0.0\n",
        "        })\n",
        "\n",
        "    # 1) Mode of sentiment types (which label appears most)\n",
        "    sentiment_counts = group[\"sentiment\"].value_counts()\n",
        "    mode_of_sentiment = sentiment_counts.idxmax()\n",
        "\n",
        "    # 2) Number of articles\n",
        "    num_articles = total_articles\n",
        "\n",
        "    # 3) Ratio of positive/negative/neutral\n",
        "    ratio_positive = sentiment_counts.get(\"Positive\", 0) / total_articles\n",
        "    ratio_negative = sentiment_counts.get(\"Negative\", 0) / total_articles\n",
        "    ratio_neutral  = sentiment_counts.get(\"Neutral\",  0) / total_articles\n",
        "\n",
        "    # 4) Average sentiment for positive, negative, neutral\n",
        "    avg_sentiment_positive = group.loc[group[\"sentiment\"] == \"Positive\", \"sentiment_logit\"].mean()\n",
        "    avg_sentiment_negative = group.loc[group[\"sentiment\"] == \"Negative\", \"sentiment_logit\"].mean()\n",
        "    avg_sentiment_neutral  = group.loc[group[\"sentiment\"] == \"Neutral\",  \"sentiment_logit\"].mean()\n",
        "\n",
        "    # 5) Weighted average sentiment:\n",
        "    #    (sum of all positive sentiments - sum of all negative sentiments) / total_articles\n",
        "    sum_pos = group.loc[group[\"sentiment\"] == \"Positive\", \"sentiment_logit\"].sum()\n",
        "    sum_neg = group.loc[group[\"sentiment\"] == \"Negative\", \"sentiment_logit\"].sum()\n",
        "    weighted_avg_sentiment = (sum_pos - sum_neg) / total_articles\n",
        "\n",
        "    # 6) Mode of sentiment score (which sentiment label has the highest sum of probabilities)\n",
        "    sum_sentiment_scores = group.groupby(\"sentiment\")[\"sentiment_logit\"].sum()\n",
        "    mode_of_sentiment_score = sum_sentiment_scores.idxmax() if not sum_sentiment_scores.empty else None\n",
        "\n",
        "    # 7) Mode of average sentiment score (which label has the highest average probability)\n",
        "    avg_sentiment_scores = group.groupby(\"sentiment\")[\"sentiment_logit\"].mean()\n",
        "    mode_of_avg_sentiment_score = avg_sentiment_scores.idxmax() if not avg_sentiment_scores.empty else None\n",
        "\n",
        "    # 8) Combined sentiment for all articles of the day:\n",
        "    combined_text = \" \".join(group[\"article_text\"].tolist()).strip()\n",
        "    # Retrieve industry from the group (all rows have the same ticker, hence same industry)\n",
        "    industry = group[\"industry\"].iloc[0] if \"industry\" in group.columns else None\n",
        "    sentiment_combined, sentiment_score_combined = get_sentiment(combined_text, industry=industry)\n",
        "\n",
        "    return pd.Series({\n",
        "        \"mode_of_sentiment\": mode_of_sentiment,\n",
        "        \"num_articles\": num_articles,\n",
        "        \"ratio_positive\": ratio_positive,\n",
        "        \"ratio_negative\": ratio_negative,\n",
        "        \"ratio_neutral\":  ratio_neutral,\n",
        "        \"avg_sentiment_positive\": avg_sentiment_positive,\n",
        "        \"avg_sentiment_negative\": avg_sentiment_negative,\n",
        "        \"avg_sentiment_neutral\":  avg_sentiment_neutral,\n",
        "        \"weighted_avg_sentiment\": weighted_avg_sentiment,\n",
        "        \"mode_of_sentiment_score\": mode_of_sentiment_score,\n",
        "        \"mode_of_avg_sentiment_score\": mode_of_avg_sentiment_score,\n",
        "        \"sentiment_combined\": sentiment_combined,\n",
        "        \"sentiment_score_combined\": sentiment_score_combined\n",
        "    })\n",
        "\n",
        "\n",
        "# -----------------------\n",
        "# 4. MAIN PROCESS\n",
        "# -----------------------\n",
        "TICKER_UNIVERSE = [\n",
        "    # 'AAPL', 'MSFT', 'NVDA', 'AVGO', 'ADBE', 'UNH', 'JNJ', 'PFE', 'MRK', 'ABBV',\n",
        "    # 'JPM', 'BAC', 'WFC', 'GS', 'MS', 'AMZN', 'TSLA', 'HD', 'MCD', 'NKE', 'GOOGL',\n",
        "    # 'META', 'DIS', 'VZ', 'CMCSA', 'PG', 'KO', 'PEP', 'WMT', 'COST', 'XOM', 'CVX',\n",
        "    'COP', 'BA', 'UNP', 'HON', 'NEE', 'DUK', 'SO', 'PLD', 'AMT', 'CCI', 'SHW', 'DOW'\n",
        "]\n",
        "\n",
        "# Directory containing all TICKER.parquet files\n",
        "input_dir = \"/content/drive/My Drive/LLM_RL_Data/stock_parquet\"\n",
        "output_dir = \"/content/drive/My Drive/LLM_RL_Data/llm_sentiments\"\n",
        "\n",
        "# Optional: warm-up pass to stabilize GPU memory\n",
        "print(\"Running warm-up pass...\")\n",
        "dummy_input = tokenizer(\"Warm-up\", return_tensors=\"pt\", padding=True, max_length=128).to(device)\n",
        "with torch.no_grad():\n",
        "    _ = model(**dummy_input)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Loop over tickers (example with ticker 'SO')\n",
        "for ticker in TICKER_UNIVERSE:\n",
        "    input_path = f\"{input_dir}/{ticker}.parquet\"\n",
        "    try:\n",
        "        df = pd.read_parquet(input_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found for {ticker}, skipping...\")\n",
        "        continue\n",
        "\n",
        "    if ticker not in df.columns:\n",
        "        print(f\"Ticker column '{ticker}' not found in {input_path}, skipping...\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nProcessing {ticker} from file: {input_path}\")\n",
        "    print(f\"Data shape: {df.shape}\")\n",
        "\n",
        "    # Convert index to column if needed\n",
        "    if df.index.name == \"trading_day\":\n",
        "        df.reset_index(inplace=True)\n",
        "\n",
        "    # Explode the list of articles so each article is its own row\n",
        "    df_exploded = df.explode(ticker, ignore_index=False).reset_index(drop=True)\n",
        "    df_exploded.rename(columns={ticker: \"article_text\"}, inplace=True)\n",
        "\n",
        "    # Fill missing article text with empty strings\n",
        "    df_exploded[\"article_text\"] = df_exploded[\"article_text\"].fillna(\"\")\n",
        "\n",
        "    # Add a column for industry using the mapping\n",
        "    industry = TICKER_INDUSTRY_MAP.get(ticker, None)\n",
        "    df_exploded[\"industry\"] = industry\n",
        "\n",
        "    # Apply get_sentiment to each article\n",
        "    sentiments, logits = [], []\n",
        "    for text in tqdm(df_exploded[\"article_text\"], desc=f\"{ticker} articles\"):\n",
        "        s_label, s_logit = get_sentiment(text, industry=industry)\n",
        "        sentiments.append(s_label)\n",
        "        logits.append(s_logit)\n",
        "\n",
        "    df_exploded[\"sentiment\"] = sentiments\n",
        "    df_exploded[\"sentiment_logit\"] = logits\n",
        "\n",
        "    # Group by trading_day to compute daily aggregates\n",
        "    grouped_results = (\n",
        "        df_exploded\n",
        "        .groupby(\"trading_day\", as_index=True)\n",
        "        .apply(aggregate_daily_sentiment)\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # Save the aggregated result for this ticker\n",
        "    ticker_output_path = f\"{output_dir}/{ticker}_sentiment_agg.parquet\"\n",
        "    grouped_results.to_parquet(ticker_output_path, index=False)\n",
        "\n",
        "    print(f\"Saved daily sentiment aggregates to {ticker_output_path}\")\n",
        "\n",
        "    # Cleanup\n",
        "    del df, df_exploded, grouped_results, sentiments, logits\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "print(\"\\nAll tickers processed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IjjG6K0nET9",
        "outputId": "65fce7c6-031c-4686-f329-0121747023ab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running warm-up pass...\n",
            "\n",
            "Processing COP from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/COP.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "COP articles: 100%|██████████| 10473/10473 [17:32<00:00,  9.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/COP_sentiment_agg.parquet\n",
            "\n",
            "Processing BA from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/BA.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "BA articles: 100%|██████████| 25586/25586 [36:52<00:00, 11.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/BA_sentiment_agg.parquet\n",
            "\n",
            "Processing UNP from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/UNP.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UNP articles: 100%|██████████| 2798/2798 [02:34<00:00, 18.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/UNP_sentiment_agg.parquet\n",
            "\n",
            "Processing HON from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/HON.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "HON articles: 100%|██████████| 3173/3173 [03:18<00:00, 16.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/HON_sentiment_agg.parquet\n",
            "\n",
            "Processing NEE from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/NEE.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "NEE articles: 100%|██████████| 2771/2771 [02:30<00:00, 18.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/NEE_sentiment_agg.parquet\n",
            "\n",
            "Processing DUK from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/DUK.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DUK articles: 100%|██████████| 2995/2995 [03:02<00:00, 16.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/DUK_sentiment_agg.parquet\n",
            "\n",
            "Processing SO from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/SO.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SO articles: 100%|██████████| 2828/2828 [02:30<00:00, 18.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/SO_sentiment_agg.parquet\n",
            "\n",
            "Processing PLD from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/PLD.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PLD articles: 100%|██████████| 2232/2232 [01:32<00:00, 24.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/PLD_sentiment_agg.parquet\n",
            "\n",
            "Processing AMT from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/AMT.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AMT articles: 100%|██████████| 2749/2749 [02:54<00:00, 15.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/AMT_sentiment_agg.parquet\n",
            "\n",
            "Processing CCI from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/CCI.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CCI articles: 100%|██████████| 2332/2332 [01:45<00:00, 22.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/CCI_sentiment_agg.parquet\n",
            "\n",
            "Processing SHW from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/SHW.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SHW articles: 100%|██████████| 2100/2100 [00:53<00:00, 39.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/SHW_sentiment_agg.parquet\n",
            "\n",
            "Processing DOW from file: /content/drive/My Drive/LLM_RL_Data/stock_parquet/DOW.parquet\n",
            "Data shape: (1848, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DOW articles: 100%|██████████| 3302/3302 [03:34<00:00, 15.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved daily sentiment aggregates to /content/drive/My Drive/LLM_RL_Data/results/DOW_sentiment_agg.parquet\n",
            "\n",
            "All tickers processed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_parquet(\"/content/drive/My Drive/LLM_RL_Data/llm_sentiments/TSLA_sentiment_agg.parquet\")\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "yIwKOXPBrMbF",
        "outputId": "ccf77863-b90d-446e-e922-c2782c86a7d8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     trading_day mode_of_sentiment  num_articles  ratio_positive  \\\n",
              "0     2018-01-01           Neutral             1        0.000000   \n",
              "1     2018-01-02           Neutral             1        0.000000   \n",
              "2     2018-01-03          Positive             4        0.750000   \n",
              "3     2018-01-04           Neutral            16        0.187500   \n",
              "4     2018-01-05           Neutral             1        0.000000   \n",
              "...          ...               ...           ...             ...   \n",
              "1843  2025-01-28           Neutral            18        0.333333   \n",
              "1844  2025-01-29           Neutral             7        0.285714   \n",
              "1845  2025-01-30           Neutral            36        0.416667   \n",
              "1846  2025-01-31          Positive            19        0.631579   \n",
              "1847  2025-02-03           Neutral             3        0.000000   \n",
              "\n",
              "      ratio_negative  ratio_neutral  avg_sentiment_positive  \\\n",
              "0           0.000000       1.000000                     NaN   \n",
              "1           0.000000       1.000000                     NaN   \n",
              "2           0.000000       0.250000                0.000547   \n",
              "3           0.062500       0.750000                0.000663   \n",
              "4           0.000000       1.000000                     NaN   \n",
              "...              ...            ...                     ...   \n",
              "1843        0.000000       0.666667                0.000386   \n",
              "1844        0.000000       0.714286                0.000556   \n",
              "1845        0.111111       0.472222                0.000574   \n",
              "1846        0.000000       0.368421                0.000793   \n",
              "1847        0.000000       1.000000                     NaN   \n",
              "\n",
              "      avg_sentiment_negative  avg_sentiment_neutral  weighted_avg_sentiment  \\\n",
              "0                        NaN               1.000000                0.000000   \n",
              "1                        NaN               1.000000                0.000000   \n",
              "2                        NaN               0.000200                0.000410   \n",
              "3                   0.000262               0.000221                0.000108   \n",
              "4                        NaN               1.000000                0.000000   \n",
              "...                      ...                    ...                     ...   \n",
              "1843                     NaN               0.000296                0.000129   \n",
              "1844                     NaN               0.000340                0.000159   \n",
              "1845                0.000222               0.000317                0.000215   \n",
              "1846                     NaN               0.000296                0.000501   \n",
              "1847                     NaN               0.000245                0.000000   \n",
              "\n",
              "     mode_of_sentiment_score mode_of_avg_sentiment_score sentiment_combined  \\\n",
              "0                    Neutral                     Neutral            Neutral   \n",
              "1                    Neutral                     Neutral            Neutral   \n",
              "2                   Positive                    Positive            Neutral   \n",
              "3                    Neutral                    Positive           Negative   \n",
              "4                    Neutral                     Neutral            Neutral   \n",
              "...                      ...                         ...                ...   \n",
              "1843                 Neutral                    Positive            Neutral   \n",
              "1844                 Neutral                    Positive           Positive   \n",
              "1845                Positive                    Positive            Neutral   \n",
              "1846                Positive                    Positive            Neutral   \n",
              "1847                 Neutral                     Neutral            Neutral   \n",
              "\n",
              "      sentiment_score_combined  \n",
              "0                     1.000000  \n",
              "1                     1.000000  \n",
              "2                     0.000301  \n",
              "3                     0.000268  \n",
              "4                     1.000000  \n",
              "...                        ...  \n",
              "1843                  0.000342  \n",
              "1844                  0.000275  \n",
              "1845                  0.000305  \n",
              "1846                  0.000281  \n",
              "1847                  0.000159  \n",
              "\n",
              "[1848 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6bf72036-301d-42ad-81a0-493eedbd1176\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>trading_day</th>\n",
              "      <th>mode_of_sentiment</th>\n",
              "      <th>num_articles</th>\n",
              "      <th>ratio_positive</th>\n",
              "      <th>ratio_negative</th>\n",
              "      <th>ratio_neutral</th>\n",
              "      <th>avg_sentiment_positive</th>\n",
              "      <th>avg_sentiment_negative</th>\n",
              "      <th>avg_sentiment_neutral</th>\n",
              "      <th>weighted_avg_sentiment</th>\n",
              "      <th>mode_of_sentiment_score</th>\n",
              "      <th>mode_of_avg_sentiment_score</th>\n",
              "      <th>sentiment_combined</th>\n",
              "      <th>sentiment_score_combined</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2018-01-01</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2018-01-02</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2018-01-03</td>\n",
              "      <td>Positive</td>\n",
              "      <td>4</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.000547</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.000410</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>0.000301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2018-01-04</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>16</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>0.062500</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.000663</td>\n",
              "      <td>0.000262</td>\n",
              "      <td>0.000221</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Negative</td>\n",
              "      <td>0.000268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2018-01-05</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1843</th>\n",
              "      <td>2025-01-28</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>18</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000386</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000296</td>\n",
              "      <td>0.000129</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>0.000342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1844</th>\n",
              "      <td>2025-01-29</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>7</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.000556</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000340</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>0.000275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1845</th>\n",
              "      <td>2025-01-30</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>36</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.472222</td>\n",
              "      <td>0.000574</td>\n",
              "      <td>0.000222</td>\n",
              "      <td>0.000317</td>\n",
              "      <td>0.000215</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>0.000305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1846</th>\n",
              "      <td>2025-01-31</td>\n",
              "      <td>Positive</td>\n",
              "      <td>19</td>\n",
              "      <td>0.631579</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.368421</td>\n",
              "      <td>0.000793</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000296</td>\n",
              "      <td>0.000501</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Positive</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>0.000281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1847</th>\n",
              "      <td>2025-02-03</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>0.000159</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1848 rows × 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6bf72036-301d-42ad-81a0-493eedbd1176')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6bf72036-301d-42ad-81a0-493eedbd1176 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6bf72036-301d-42ad-81a0-493eedbd1176');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4b6dea9d-763c-480e-9664-fda7015b2861\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4b6dea9d-763c-480e-9664-fda7015b2861')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4b6dea9d-763c-480e-9664-fda7015b2861 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_90fd723b-92e8-4e0a-b298-db7d56326ade\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_90fd723b-92e8-4e0a-b298-db7d56326ade button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1848,\n  \"fields\": [\n    {\n      \"column\": \"trading_day\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2018-01-01\",\n        \"max\": \"2025-02-03\",\n        \"num_unique_values\": 1848,\n        \"samples\": [\n          \"2019-05-07\",\n          \"2022-12-13\",\n          \"2023-10-27\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mode_of_sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Neutral\",\n          \"Positive\",\n          \"Negative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_articles\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11,\n        \"min\": 1,\n        \"max\": 74,\n        \"num_unique_values\": 66,\n        \"samples\": [\n          52,\n          69,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ratio_positive\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2432498140949735,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 295,\n        \"samples\": [\n          0.32432432432432434,\n          0.6086956521739131,\n          0.26666666666666666\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ratio_negative\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10002094043095579,\n        \"min\": 0.0,\n        \"max\": 0.75,\n        \"num_unique_values\": 173,\n        \"samples\": [\n          0.13793103448275862,\n          0.043478260869565216,\n          0.2702702702702703\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ratio_neutral\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.24267004592152694,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 297,\n        \"samples\": [\n          0.48484848484848486,\n          0.7631578947368421,\n          0.38461538461538464\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_sentiment_positive\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00018308820897548783,\n        \"min\": 1.0386546591689694e-06,\n        \"max\": 0.0014427256537601352,\n        \"num_unique_values\": 1590,\n        \"samples\": [\n          0.0003982342022936791,\n          0.0007551725539087784,\n          0.0005191078442814094\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_sentiment_negative\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.307885870722794e-05,\n        \"min\": 4.4042535591870546e-05,\n        \"max\": 0.0005816139164380729,\n        \"num_unique_values\": 692,\n        \"samples\": [\n          0.0001835968578234315,\n          0.00023917329963296652,\n          0.0002514857624191791\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"avg_sentiment_neutral\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14543375613939236,\n        \"min\": 5.0593967898748815e-05,\n        \"max\": 1.0,\n        \"num_unique_values\": 1766,\n        \"samples\": [\n          0.00022145292960986908,\n          0.00022277170501183718,\n          0.00030689100967720153\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"weighted_avg_sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00016225623083686807,\n        \"min\": -0.00021115055278642103,\n        \"max\": 0.0012102961773052812,\n        \"num_unique_values\": 1634,\n        \"samples\": [\n          0.00015362056393104678,\n          2.9033001959072175e-05,\n          0.0001365555591681706\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mode_of_sentiment_score\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Neutral\",\n          \"Positive\",\n          \"Negative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mode_of_avg_sentiment_score\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Neutral\",\n          \"Positive\",\n          \"Negative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment_combined\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Neutral\",\n          \"Negative\",\n          \"Positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment_score_combined\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14371528814365256,\n        \"min\": 1.1128862276166274e-08,\n        \"max\": 1.0,\n        \"num_unique_values\": 1810,\n        \"samples\": [\n          0.0002648174995556474,\n          0.00028387492056936026,\n          0.0002853250771295279\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OLD CODE"
      ],
      "metadata": {
        "id": "2QbRYCKDiVnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Define function for sentiment analysis\n",
        "def get_sentiment(text):\n",
        "    if not text:  # If the text is empty, return neutral sentiment\n",
        "        return \"Neutral\", 1.0  # Neutral with logit probability 1.0\n",
        "    len_text = min(len(text),2000)\n",
        "    text = text[:len_text]\n",
        "    # Define the prompt for the model\n",
        "    prompt = f'''Instruction: What is the sentiment of this news? Please choose an answer from [Positive, Negative, Neutral].\\nInput: {text}\\nAnswer: '''\n",
        "\n",
        "    # Tokenize directly on the GPU for efficiency\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, max_length=128).to(device)\n",
        "\n",
        "    # Forward pass on GPU\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get logits for the last token and move them back to CPU\n",
        "    logits = outputs.logits[:, -1, :].to(\"cpu\")\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    # Class tokens for Positive, Negative, Neutral\n",
        "    class_tokens = tokenizer([\"Positive\", \"Negative\", \"Neutral\"], add_special_tokens=False)[\"input_ids\"]\n",
        "    class_probs = {tokenizer.decode(token_id): probs[0, token_id].item() for token_id in class_tokens}\n",
        "\n",
        "    # Get the most probable sentiment\n",
        "    sentiment = max(class_probs, key=class_probs.get)\n",
        "\n",
        "    # Clear intermediate variables\n",
        "    del inputs, outputs, logits, probs\n",
        "    torch.cuda.empty_cache()\n",
        "    return sentiment, class_probs[sentiment]\n"
      ],
      "metadata": {
        "id": "KRyLNU7qaZpN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N126rvUKPGjv"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "batch_size = 500  # Number of rows per batch\n",
        "output_dir = '/content/drive/My Drive/LLM_RL_Data/results'  # Directory for individual ticker files\n",
        "\n",
        "# Process the parquet files\n",
        "file_paths = [\n",
        "    '/content/drive/My Drive/LLM_RL_Data/TICKER.parquet'\n",
        "]\n",
        "\n",
        "# Warm-up pass\n",
        "print(\"Running warm-up pass to stabilize GPU memory...\")\n",
        "dummy_input = tokenizer(\"Warm-up\", return_tensors=\"pt\", padding=True, max_length=128).to(device)\n",
        "with torch.no_grad():\n",
        "    _ = model(**dummy_input)\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Process each file\n",
        "for input_path in file_paths:\n",
        "    year = input_path.split('_')[-1].split('.')[0]  # Extract year from filename\n",
        "    print(f\"\\nProcessing file: {input_path} (Year: {year})\")\n",
        "\n",
        "    # Load the parquet file\n",
        "    df = pd.read_parquet(input_path)\n",
        "\n",
        "    for ticker in ['TSLA']:\n",
        "        print(f\"\\nProcessing sentiment for ticker: {ticker}\")\n",
        "\n",
        "        reddit_col = f\"{ticker}_reddit\"\n",
        "        sentiment_col = f\"{ticker}_sentiment\"\n",
        "        logit_col = f\"{ticker}_logit\"\n",
        "\n",
        "        # Initialize lists for sentiments and logits\n",
        "        sentiments = []\n",
        "        logits = []\n",
        "\n",
        "        # Process DataFrame in batches\n",
        "        num_batches = (len(df) + batch_size - 1) // batch_size  # Calculate total number of batches\n",
        "        for batch_num in tqdm(range(num_batches), desc=f\"Ticker: {ticker} Batches\"):\n",
        "            # Extract batch\n",
        "            start_idx = batch_num * batch_size\n",
        "            end_idx = min((batch_num + 1) * batch_size, len(df))\n",
        "            batch_df = df.iloc[start_idx:end_idx]\n",
        "\n",
        "            # Process each row in the batch\n",
        "            batch_sentiments = []\n",
        "            batch_logits = []\n",
        "            for text in batch_df[reddit_col].fillna(\"\").tolist():\n",
        "                sentiment, logit = get_sentiment(text)  # GPU inference\n",
        "                batch_sentiments.append(sentiment)\n",
        "                batch_logits.append(logit)\n",
        "\n",
        "            # Append batch results to the main lists\n",
        "            sentiments.extend(batch_sentiments)\n",
        "            logits.extend(batch_logits)\n",
        "\n",
        "            # Clear GPU memory periodically\n",
        "            del batch_df, batch_sentiments, batch_logits\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        # Create and save a DataFrame for this ticker\n",
        "        ticker_df = pd.DataFrame({\n",
        "            'timestamp': df['timestamp'],  # Assuming there's a timestamp column\n",
        "            reddit_col: df[reddit_col],\n",
        "            sentiment_col: sentiments,\n",
        "            logit_col: logits\n",
        "        })\n",
        "\n",
        "        # Save the ticker DataFrame to a separate parquet file\n",
        "        ticker_output_path = f\"{output_dir}/{ticker}_{year}.parquet\"\n",
        "        ticker_df.to_parquet(ticker_output_path, index=False)\n",
        "        print(f\"Saved results for {ticker} to {ticker_output_path}\")\n",
        "\n",
        "        # Clear memory for the ticker\n",
        "        del ticker_df, sentiments, logits\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # Clear memory for the file\n",
        "    del df\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    print(f\"Cleared GPU cache and unnecessary variables for: {input_path}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2QbRYCKDiVnv"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "96d3ed4927f842f38644a65d4fbae971": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f0b69ec784a43fa9b2589aeb1592a15",
              "IPY_MODEL_426254dc10734a26b81c965c4e3af6ea",
              "IPY_MODEL_64023af9d137451fbbfd5fd6cd4d7a3a"
            ],
            "layout": "IPY_MODEL_9a202611185542f290cdbec6fc4ebfdb"
          }
        },
        "0f0b69ec784a43fa9b2589aeb1592a15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fcaf3432e1b4563aa0698ac1df0d2d5",
            "placeholder": "​",
            "style": "IPY_MODEL_fa4851ee3d094a0386ddddd8815d1bb7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "426254dc10734a26b81c965c4e3af6ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e55f713fe124503b5ad9cda848e2b0d",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fc8bfd64144e46f0a13450e402d56b27",
            "value": 4
          }
        },
        "64023af9d137451fbbfd5fd6cd4d7a3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e81828912a944c18cf64b2e2f09fb63",
            "placeholder": "​",
            "style": "IPY_MODEL_24a5133c905c4765911d0dc91ae7599d",
            "value": " 4/4 [00:09&lt;00:00,  2.10s/it]"
          }
        },
        "9a202611185542f290cdbec6fc4ebfdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fcaf3432e1b4563aa0698ac1df0d2d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa4851ee3d094a0386ddddd8815d1bb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e55f713fe124503b5ad9cda848e2b0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc8bfd64144e46f0a13450e402d56b27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e81828912a944c18cf64b2e2f09fb63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24a5133c905c4765911d0dc91ae7599d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}